{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLiMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = Path(\"data/blimp_with_targets\")\n",
    "target_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"anaphor_gender_agreement\", \"anaphor_number_agreement\"]\n",
    "\n",
    "for file_name in file_names:\n",
    "    content = pd.read_json(\n",
    "        path_or_buf=f\"data/blimp/{file_name}.jsonl\",\n",
    "        lines=True\n",
    "    )\n",
    "\n",
    "    targets, targets_phrases = [], []\n",
    "    for row in content.itertuples():\n",
    "        \n",
    "        doc = nlp(row.sentence_good)\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text == row.one_prefix_word_good:\n",
    "                target_token = token\n",
    "\n",
    "        target = []\n",
    "        for cluster in doc._.coref_clusters:\n",
    "            for mention in cluster:\n",
    "                if mention.root.text == target_token.text:\n",
    "                    target = [token for token in cluster if token.text != target_token.text]\n",
    "                    break\n",
    "                \n",
    "        if len(target) == 1:\n",
    "            target = target[0]\n",
    "            targets.append(target.root.text)\n",
    "            targets_phrases.append(target.text)\n",
    "        else:\n",
    "            targets.append(\"NA\")\n",
    "            targets_phrases.append(\"NA\")\n",
    "            \n",
    "        \n",
    "    content[\"target\"] = targets\n",
    "    content[\"target_phrase\"] = targets_phrases\n",
    "\n",
    "    content.to_csv(target_path / f\"{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main verb\n",
    "# Argument Structure (asp)\n",
    "\n",
    "file_name = \"animate_subject_passive\"\n",
    "\n",
    "content = pd.read_json(\n",
    "    path_or_buf=f\"data/blimp/{file_name}.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "content[\"target\"] = [\n",
    "    list(textacy.extract.matches.token_matches(\n",
    "        nlp(sent),\n",
    "        patterns=[{\"POS\": \"VERB\"}]\n",
    "    ))[0][0]\n",
    "    for sent in content.sentence_good\n",
    "]\n",
    "\n",
    "content.to_csv(target_path / f\"{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# det noun\n",
    "# Determiner-Noun Agreement\n",
    "\n",
    "file_names = [\n",
    "    \"determiner_noun_agreement_1\", \"determiner_noun_agreement_irregular_1\",\n",
    "    \"determiner_noun_agreement_with_adjective_1\", \"determiner_noun_agreement_with_adj_irregular_1\"]\n",
    "\n",
    "for file_name in file_names:\n",
    "    content = pd.read_json(\n",
    "        path_or_buf=f\"data/blimp/{file_name}.jsonl\",\n",
    "        lines=True\n",
    "    )\n",
    "    \n",
    "    targets = []\n",
    "    \n",
    "    for row in content.itertuples():\n",
    "        \n",
    "        tokenized_sent = row.sentence_good.split(\" \")     \n",
    "        for token in [\"this\", \"that\", \"those\", \"these\"]:\n",
    "            if token in tokenized_sent:\n",
    "                targets.append(token)\n",
    "                break\n",
    "\n",
    "    assert len(targets) == len(content)\n",
    "    \n",
    "    content[\"target\"] = targets\n",
    "    \n",
    "    content.to_csv(target_path / f\"{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npi\n",
    "# NPI Licensing\n",
    "\n",
    "file_name = \"npi_present_1\"\n",
    "\n",
    "content = pd.read_json(\n",
    "    path_or_buf=f\"data/blimp/{file_name}.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "content[\"target\"] = \"Even\"\n",
    "\n",
    "content.to_csv(target_path / f\"{file_name}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Verb Agreement (SVA and darn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = Path(\"data/sva_with_targets\")\n",
    "target_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"subj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29985/29985 [04:01<00:00, 124.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# subj verb\n",
    "# Subject-Verb Agreement\n",
    "\n",
    "from extract_explanations import read_sva_dataset\n",
    "\n",
    "dataset = 'sva'\n",
    "        \n",
    "file_names = [\"distractor_agreement_relational_noun\"]\n",
    "\n",
    "file_names = ['lgd_dataset']\n",
    "\n",
    "for file_name in file_names:\n",
    "    \n",
    "    if dataset == 'blimp':\n",
    "        content = pd.read_json(\n",
    "            path_or_buf=f\"data/{dataset}/{file_name}.jsonl\",\n",
    "            lines=True\n",
    "        )\n",
    "    else:\n",
    "        content = pd.read_csv('./data/sva/lgd_dataset.tsv', sep='\\t', names=[\"num_attractors\", \"sentence_good\", \"one_prefix_prefix\", \"one_prefix_word_good\", \"one_prefix_word_bad\"])\n",
    "    \n",
    "    targets, targets_phrases = [], []\n",
    "    for sent in tqdm(content.sentence_good):\n",
    "        doc = nlp(sent)\n",
    "        \n",
    "        subject_phrase = get_subject_phrase(doc)\n",
    "        \n",
    "        if subject_phrase:\n",
    "            targets.append(subject_phrase.root.text)\n",
    "            targets_phrases.append(subject_phrase.text)\n",
    "        else:\n",
    "            targets.append(\"NA\")\n",
    "            targets_phrases.append(\"NA\")\n",
    "        \n",
    "    content[\"target\"] = targets\n",
    "    content[\"target_phrase\"] = targets_phrases\n",
    "\n",
    "    \n",
    "    content.to_csv(target_path / f\"{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_attractors</th>\n",
       "      <th>sentence_good</th>\n",
       "      <th>one_prefix_prefix</th>\n",
       "      <th>one_prefix_word_good</th>\n",
       "      <th>one_prefix_word_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a 12th-century commentary on periegetes by eus...</td>\n",
       "      <td>a 12th-century commentary on periegetes by eus...</td>\n",
       "      <td>compares</td>\n",
       "      <td>compare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a 13-year boy named toby lolness , who is just...</td>\n",
       "      <td>a 13-year boy named toby lolness , who is just...</td>\n",
       "      <td>lives</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>a 16-year-old , second-year high school studen...</td>\n",
       "      <td>a 16-year-old , second-year high school studen...</td>\n",
       "      <td>resembles</td>\n",
       "      <td>resemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a 1770s map of philadelphia 's naval defenses ...</td>\n",
       "      <td>a 1770s map of philadelphia 's naval defenses ...</td>\n",
       "      <td>shows</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>a 1794 plan of the 'castle ' exists , this onl...</td>\n",
       "      <td>a 1794 plan of the 'castle ' exists , this onl...</td>\n",
       "      <td>shows</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29977</th>\n",
       "      <td>1</td>\n",
       "      <td>zinc-finger genes , particularly those that in...</td>\n",
       "      <td>zinc-finger genes , particularly those that in...</td>\n",
       "      <td>exist</td>\n",
       "      <td>exists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29979</th>\n",
       "      <td>1</td>\n",
       "      <td>zip file format for all users , the appnote pr...</td>\n",
       "      <td>zip file format for all users , the appnote **...</td>\n",
       "      <td>provides</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29980</th>\n",
       "      <td>1</td>\n",
       "      <td>zirconium powder can cause irritation , but on...</td>\n",
       "      <td>zirconium powder can cause irritation , but on...</td>\n",
       "      <td>requires</td>\n",
       "      <td>require</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29981</th>\n",
       "      <td>1</td>\n",
       "      <td>zöllner 's illusion and the café wall illusion...</td>\n",
       "      <td>zöllner 's illusion and the café wall illusion...</td>\n",
       "      <td>causes</td>\n",
       "      <td>cause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29984</th>\n",
       "      <td>1</td>\n",
       "      <td>zubek 's impressive vision and original compos...</td>\n",
       "      <td>zubek 's impressive vision and original compos...</td>\n",
       "      <td>succeeds</td>\n",
       "      <td>succeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24299 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_attractors                                      sentence_good  \\\n",
       "0                   1  a 12th-century commentary on periegetes by eus...   \n",
       "1                   1  a 13-year boy named toby lolness , who is just...   \n",
       "2                   1  a 16-year-old , second-year high school studen...   \n",
       "3                   1  a 1770s map of philadelphia 's naval defenses ...   \n",
       "4                   1  a 1794 plan of the 'castle ' exists , this onl...   \n",
       "...               ...                                                ...   \n",
       "29977               1  zinc-finger genes , particularly those that in...   \n",
       "29979               1  zip file format for all users , the appnote pr...   \n",
       "29980               1  zirconium powder can cause irritation , but on...   \n",
       "29981               1  zöllner 's illusion and the café wall illusion...   \n",
       "29984               1  zubek 's impressive vision and original compos...   \n",
       "\n",
       "                                       one_prefix_prefix one_prefix_word_good  \\\n",
       "0      a 12th-century commentary on periegetes by eus...             compares   \n",
       "1      a 13-year boy named toby lolness , who is just...                lives   \n",
       "2      a 16-year-old , second-year high school studen...            resembles   \n",
       "3      a 1770s map of philadelphia 's naval defenses ...                shows   \n",
       "4      a 1794 plan of the 'castle ' exists , this onl...                shows   \n",
       "...                                                  ...                  ...   \n",
       "29977  zinc-finger genes , particularly those that in...                exist   \n",
       "29979  zip file format for all users , the appnote **...             provides   \n",
       "29980  zirconium powder can cause irritation , but on...             requires   \n",
       "29981  zöllner 's illusion and the café wall illusion...               causes   \n",
       "29984  zubek 's impressive vision and original compos...             succeeds   \n",
       "\n",
       "      one_prefix_word_bad  \n",
       "0                 compare  \n",
       "1                    live  \n",
       "2                resemble  \n",
       "3                    show  \n",
       "4                    show  \n",
       "...                   ...  \n",
       "29977              exists  \n",
       "29979             provide  \n",
       "29980             require  \n",
       "29981               cause  \n",
       "29984             succeed  \n",
       "\n",
       "[24299 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = pd.read_csv('./data/sva/lgd_dataset.tsv', sep='\\t', names=[\"num_attractors\", \"sentence_good\", \"one_prefix_prefix\", \"one_prefix_word_good\", \"one_prefix_word_bad\"])\n",
    "content[content.num_attractors == 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOI\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "target_path = Path(\"data/ioi_with_targets\")\n",
    "target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load IOI dataset from HuggingFace\n",
    "dataset = load_dataset(\"fahamu/ioi\")\n",
    "content = dataset['train'].select(np.arange(0,1000))\n",
    "file_name = 'ioi_dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [00:11<00:00, 90.00it/s]\n"
     ]
    }
   ],
   "source": [
    "proper_names = []\n",
    "targets = []\n",
    "# Prefix\n",
    "one_prefix_prefix = []\n",
    "one_prefix_word_good = []\n",
    "one_prefix_word_bad = []\n",
    "\n",
    "content = pd.DataFrame(content)\n",
    "for counter, sent in enumerate(tqdm(content['ioi_sentences'])):\n",
    "    doc = nlp(sent)\n",
    "    one_prefix_prefix.append(' '.join(sent.split()[:-1]))\n",
    "    one_prefix_word_good.append(sent.split()[-1])\n",
    "    # Last word is target\n",
    "    targets.append(sent.split()[-1])\n",
    "\n",
    "    first_word = doc[0].text\n",
    "    last_name = doc[-1].text\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        if token.pos_ == \"PROPN\":\n",
    "            if token.text != first_word and token.text != last_name:\n",
    "                one_prefix_word_bad.append(token.text)\n",
    "                break\n",
    "\n",
    "content.rename(columns={'ioi_sentences': 'Sentence_good'}, inplace=True)\n",
    "content[\"one_prefix_prefix\"] = one_prefix_prefix\n",
    "content[\"one_prefix_word_good\"] = one_prefix_word_good\n",
    "content[\"one_prefix_word_bad\"] = one_prefix_word_bad\n",
    "content[\"target\"] = targets\n",
    "\n",
    "\n",
    "content.to_csv(target_path / f\"{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_good</th>\n",
       "      <th>one_prefix_prefix</th>\n",
       "      <th>one_prefix_word_good</th>\n",
       "      <th>one_prefix_word_bad</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friends Juana and Kristi found a mango at the ...</td>\n",
       "      <td>Friends Juana and Kristi found a mango at the ...</td>\n",
       "      <td>Juana</td>\n",
       "      <td>Kristi</td>\n",
       "      <td>Juana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Then, Yvette and Angie were working at the mou...</td>\n",
       "      <td>Then, Yvette and Angie were working at the mou...</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Yvette</td>\n",
       "      <td>Angie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After Doris and Marsha went to the mountain, M...</td>\n",
       "      <td>After Doris and Marsha went to the mountain, M...</td>\n",
       "      <td>Doris</td>\n",
       "      <td>Marsha</td>\n",
       "      <td>Doris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>While Bernadette and Harriet were commuting to...</td>\n",
       "      <td>While Bernadette and Harriet were commuting to...</td>\n",
       "      <td>Harriet</td>\n",
       "      <td>Bernadette</td>\n",
       "      <td>Harriet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afterwards, Ginger and Bernadette went to the ...</td>\n",
       "      <td>Afterwards, Ginger and Bernadette went to the ...</td>\n",
       "      <td>Ginger</td>\n",
       "      <td>Bernadette</td>\n",
       "      <td>Ginger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Afterwards, Gwen and Chelsea went to the beach...</td>\n",
       "      <td>Afterwards, Gwen and Chelsea went to the beach...</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>Gwen</td>\n",
       "      <td>Chelsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>The river Amanda and Carol went to had a canta...</td>\n",
       "      <td>The river Amanda and Carol went to had a canta...</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>Carol</td>\n",
       "      <td>Amanda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>While Sonia and Marian were commuting to the t...</td>\n",
       "      <td>While Sonia and Marian were commuting to the t...</td>\n",
       "      <td>Marian</td>\n",
       "      <td>Sonia</td>\n",
       "      <td>Marian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The store Janet and Leigh went to had a apple....</td>\n",
       "      <td>The store Janet and Leigh went to had a apple....</td>\n",
       "      <td>Janet</td>\n",
       "      <td>Leigh</td>\n",
       "      <td>Janet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>After Yvette and Diana went to the desert, Yve...</td>\n",
       "      <td>After Yvette and Diana went to the desert, Yve...</td>\n",
       "      <td>Diana</td>\n",
       "      <td>Yvette</td>\n",
       "      <td>Diana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sentence_good  \\\n",
       "0    Friends Juana and Kristi found a mango at the ...   \n",
       "1    Then, Yvette and Angie were working at the mou...   \n",
       "2    After Doris and Marsha went to the mountain, M...   \n",
       "3    While Bernadette and Harriet were commuting to...   \n",
       "4    Afterwards, Ginger and Bernadette went to the ...   \n",
       "..                                                 ...   \n",
       "995  Afterwards, Gwen and Chelsea went to the beach...   \n",
       "996  The river Amanda and Carol went to had a canta...   \n",
       "997  While Sonia and Marian were commuting to the t...   \n",
       "998  The store Janet and Leigh went to had a apple....   \n",
       "999  After Yvette and Diana went to the desert, Yve...   \n",
       "\n",
       "                                     one_prefix_prefix one_prefix_word_good  \\\n",
       "0    Friends Juana and Kristi found a mango at the ...                Juana   \n",
       "1    Then, Yvette and Angie were working at the mou...                Angie   \n",
       "2    After Doris and Marsha went to the mountain, M...                Doris   \n",
       "3    While Bernadette and Harriet were commuting to...              Harriet   \n",
       "4    Afterwards, Ginger and Bernadette went to the ...               Ginger   \n",
       "..                                                 ...                  ...   \n",
       "995  Afterwards, Gwen and Chelsea went to the beach...              Chelsea   \n",
       "996  The river Amanda and Carol went to had a canta...               Amanda   \n",
       "997  While Sonia and Marian were commuting to the t...               Marian   \n",
       "998  The store Janet and Leigh went to had a apple....                Janet   \n",
       "999  After Yvette and Diana went to the desert, Yve...                Diana   \n",
       "\n",
       "    one_prefix_word_bad   target  \n",
       "0                Kristi    Juana  \n",
       "1                Yvette    Angie  \n",
       "2                Marsha    Doris  \n",
       "3            Bernadette  Harriet  \n",
       "4            Bernadette   Ginger  \n",
       "..                  ...      ...  \n",
       "995                Gwen  Chelsea  \n",
       "996               Carol   Amanda  \n",
       "997               Sonia   Marian  \n",
       "998               Leigh    Janet  \n",
       "999              Yvette    Diana  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3405deede7389afc92991d2191c9c62758e795e1f325e882e07f8f2c49f02d5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
